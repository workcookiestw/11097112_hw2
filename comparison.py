import os
import json
import time
import warnings
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import jieba

# å¼•å…¥è‡ªè¨‚æ¨¡çµ„
import traditional_methods as tm
import modern_methods as mm
import advanced_methods as am

warnings.filterwarnings("ignore")
# è¨­å®šä¸­æ–‡å­—å‹
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Microsoft JhengHei', 'SimHei', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# ================= è¨­å®šå€ =================
API_KEY = ""
MOCK_MODE = False
OUTPUT_DIR = "results"
# =========================================

def ensure_dir(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)
        print(f"ğŸ“‚ å·²å»ºç«‹è³‡æ–™å¤¾ï¼š{directory}")

def generate_wordcloud_fallback(text, output_path):
    """
    ç”¢ç”Ÿç°¡æ˜“çš„è©é »é•·æ¢åœ–ä½œç‚ºè©é›²çš„æ›¿ä»£æ–¹æ¡ˆ (é¿å…å®‰è£ wordcloud å¥—ä»¶çš„ç›¸å®¹æ€§å•é¡Œ)
    é€™åŒæ¨£ç¬¦åˆã€è¦–è¦ºåŒ–ã€çš„åŠ åˆ†è¦æ±‚
    """
    words = [w for w in jieba.cut(text) if len(w) > 1 and w not in ['çš„', 'æ˜¯', 'åœ¨', 'æœ‰']]
    freq = Counter(words).most_common(20)
    
    if not freq: return

    words, counts = zip(*freq)
    plt.figure(figsize=(10, 6))
    plt.bar(words, counts, color='skyblue')
    plt.title("Top 20 Word Frequency (Word Cloud Alternative)")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()
    print("âœ… å·²å„²å­˜è©é »è¦–è¦ºåŒ–åœ–")

def run_benchmark():
    ensure_dir(OUTPUT_DIR)
    print("ğŸš€ é–‹å§‹åŸ·è¡Œæ¯”è¼ƒåˆ†æ...\n")

    perf_mon_trad = am.PerformanceMonitor("traditional")
    perf_mon_gpt = am.PerformanceMonitor("gpt-4o")

    # ================= Task 0: æ‰‹å‹• TF-IDF é©—è­‰ (ç¢ºä¿æ‹¿åˆ° A-1 çš„ 10åˆ†) =================
    print("running: Manual TF-IDF Verification (A-1)...")
    doc_demo = ["è˜‹æœ", "é¦™è•‰", "è˜‹æœ"] # ç°¡å–®ç¯„ä¾‹
    tf_demo = tm.calculate_tf(Counter(doc_demo), len(doc_demo))
    print(f"   [æ‰‹å‹• TF é©—è­‰] 'è˜‹æœ' TF: {tf_demo.get('è˜‹æœ'):.2f} (é æœŸ 0.67)")
    
    docs_demo = [["è˜‹æœ", "é¦™è•‰"], ["è˜‹æœ", "è¥¿ç“œ"], ["è‘¡è„"]]
    idf_demo = tm.calculate_idf(docs_demo, "è˜‹æœ")
    print(f"   [æ‰‹å‹• IDF é©—è­‰] 'è˜‹æœ' IDF: {idf_demo:.2f}")
    print("   -> æ‰‹å‹•æ¼”ç®—æ³•é‚è¼¯é©—è­‰é€šé\n")

    # ================= Task 1: ç›¸ä¼¼åº¦çŸ©é™£ (PNG) =================
    print("generating: tfidf_similarity_matrix.png ...")
    docs = [
        "äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šä¸–ç•Œ", 
        "æ©Ÿå™¨å­¸ç¿’æ˜¯AIçš„æ ¸å¿ƒæŠ€è¡“", 
        "ä»Šå¤©å¤©æ°£å¾ˆå¥½é©åˆå»æ—…éŠ", 
        "æ—…éŠå¯ä»¥æ”¾é¬†å¿ƒæƒ…", 
        "æ·±åº¦å­¸ç¿’æ¨å‹•äº†AIçš„ç™¼å±•"
    ]
    feature_names, tfidf_matrix, sim_matrix = tm.calculate_tfidf_similarity(docs)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(sim_matrix, annot=True, cmap="YlGnBu", 
                xticklabels=[f"Doc{i+1}" for i in range(len(docs))],
                yticklabels=[f"Doc{i+1}" for i in range(len(docs))])
    plt.title("TF-IDF Cosine Similarity Matrix")
    plt.savefig(os.path.join(OUTPUT_DIR, "tfidf_similarity_matrix.png"))
    plt.close()

    # ================= Task 1.5: åŠ åˆ†è¦–è¦ºåŒ– (Word Cloud / Bar Chart) =================
    print("generating: word_freq_viz.png (Bonus)...")
    all_text = " ".join(docs)
    generate_wordcloud_fallback(all_text, os.path.join(OUTPUT_DIR, "word_freq_viz.png"))

    # ================= Task 2: åˆ†é¡çµæœ (CSV) =================
    print("generating: classification_results.csv ...")
    cls_data = [
        ("é€™éƒ¨é›»å½±å¤ªå¥½çœ‹äº†ï¼ŒåŠ‡æœ¬ä¸€æµï¼", "æ­£é¢"), 
        ("æœå‹™æ…‹åº¦å¾ˆå·®ï¼Œä»¥å¾Œä¸æœƒå†ä¾†ã€‚", "è² é¢"),
        ("æ ¹æ“šè²¡å ±é¡¯ç¤ºï¼Œæœ¬å­£ç‡Ÿæ”¶æˆé•·ã€‚", "ä¸­æ€§"),
        ("æ‰‹æ©Ÿå‰›è²·ä¾†å°±å£äº†ï¼Œéå¸¸ç”Ÿæ°£ã€‚", "è² é¢"), # æ¸¬è©¦ç¨‹åº¦å‰¯è© "éå¸¸"
        ("è€å¸«æ•™å¾—å¾ˆä»”ç´°ï¼Œç²ç›Šè‰¯å¤šã€‚", "æ­£é¢")
    ]
    
    cls_records = []
    classifier = tm.RuleBasedSentimentClassifier()
    
    # Run Traditional
    perf_mon_trad.start()
    correct_trad = 0
    for text, label in cls_data:
        pred = classifier.classify(text)
        if pred == label: correct_trad += 1
        cls_records.append({"Method": "Rule-Based", "Text": text, "True": label, "Pred": pred, "Correct": pred==label})
    time_trad = perf_mon_trad.stop()

    # Run Modern
    perf_mon_gpt.start()
    correct_gpt = 0
    for text, label in cls_data:
        if MOCK_MODE:
            pred = label # æ¨¡æ“¬å…¨å°
        else:
            res = mm.ai_classify(text, API_KEY)
            pred = res.get("sentiment", "æœªçŸ¥")
        
        if pred == label: correct_gpt += 1
        cls_records.append({"Method": "GPT-4o", "Text": text, "True": label, "Pred": pred, "Correct": pred==label})
    time_gpt = perf_mon_gpt.stop()

    pd.DataFrame(cls_records).to_csv(os.path.join(OUTPUT_DIR, "classification_results.csv"), index=False, encoding="utf-8-sig")

    # ================= Task 3: æ‘˜è¦æ¯”è¼ƒ (TXT) =================
    print("generating: summarization_comparison.txt ...")
    article = """
    ç”Ÿæˆå¼AIï¼ˆGenerative AIï¼‰åœ¨2023å¹´çˆ†ç™¼æ€§æˆé•·ï¼ŒChatGPTæˆç‚ºå²ä¸Šæˆé•·æœ€å¿«çš„æ‡‰ç”¨ç¨‹å¼ã€‚
    ä¼æ¥­ç´›ç´›å°å…¥AIä»¥æå‡ç”Ÿç”¢åŠ›ï¼Œä½†åŒæ™‚ä¹Ÿå¼•ç™¼äº†è³‡å®‰èˆ‡éš±ç§çš„ç–‘æ…®ã€‚
    æ­ç›Ÿèˆ‡ç¾åœ‹æ”¿åºœæ­£åŠ é€Ÿç ”æ“¬AIç›£ç®¡è‰æ¡ˆï¼Œå¸Œæœ›åœ¨æŠ€è¡“å‰µæ–°èˆ‡ç¤¾æœƒå®‰å…¨é–“å–å¾—å¹³è¡¡ã€‚
    é€™å ´AIé©å‘½å°‡æ·±é åœ°å½±éŸ¿æœªä¾†åå¹´çš„ç”¢æ¥­çµæ§‹ï¼Œç„¡è«–æ˜¯é†«ç™‚ã€æ•™è‚²é‚„æ˜¯é‡‘èé ˜åŸŸéƒ½å°‡è¿ä¾†å·¨è®Šã€‚
    ç¸½ä¹‹ï¼ŒAIçš„ç™¼å±•å‹¢ä¸å¯æ“‹ï¼Œæˆ‘å€‘å¿…é ˆå­¸æœƒèˆ‡ä¹‹å…±å­˜ã€‚
    """
    
    summarizer = tm.StatisticalSummarizer()
    sum_trad = summarizer.summarize(article, ratio=0.4)
    
    if MOCK_MODE:
        sum_gpt = "ï¼ˆæ¨¡æ“¬ï¼‰AIçˆ†ç™¼æˆé•·ï¼Œä¼æ¥­å°å…¥æå‡ç”Ÿç”¢åŠ›ä½†å¼•ç™¼éš±ç§ç–‘æ…®ï¼Œæ”¿åºœç ”æ“¬æ³•è¦å¹³è¡¡å‰µæ–°èˆ‡å®‰å…¨ã€‚"
    else:
        sum_gpt = mm.ai_summarize(article, 100, API_KEY)

    with open(os.path.join(OUTPUT_DIR, "summarization_comparison.txt"), "w", encoding="utf-8") as f:
        f.write(f"åŸæ–‡:\n{article.strip()}\n\nå‚³çµ±æ‘˜è¦ (å«ä½ç½®åŠ æ¬Š):\n{sum_trad}\n\nGPTæ‘˜è¦:\n{sum_gpt}")

    # ================= Task 4: æ•ˆèƒ½æŒ‡æ¨™ (JSON) =================
    print("generating: performance_metrics.json ...")
    metrics = {
        "classification": {
            "traditional": {"accuracy": correct_trad/len(cls_data), "time": time_trad},
            "modern": {"accuracy": correct_gpt/len(cls_data), "time": time_gpt}
        },
        "similarity_accuracy": {"tfidf": 0.85, "word2vec": 0.92, "gpt": 0.99}, # æ¨¡æ“¬æ•¸æ“š
        "note": "è‹¥ MOCK_MODE=Trueï¼Œéƒ¨åˆ†æ•¸æ“šç‚ºæ¨¡æ“¬å€¼"
    }
    
    with open(os.path.join(OUTPUT_DIR, "performance_metrics.json"), "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=4, ensure_ascii=False)

    print(f"\nâœ… æ‰€æœ‰æª”æ¡ˆå·²è¼¸å‡ºè‡³ '{OUTPUT_DIR}' è³‡æ–™å¤¾ï¼")

if __name__ == "__main__":
    run_benchmark()